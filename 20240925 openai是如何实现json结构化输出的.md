### OpenAI是如何实现JSON Schema输出的？
总的来说，官方提升模型的同时采用了工程化技巧来解决这件事情。这里我们不细说了，我们直接翻译如下：

#### 内部工作机制
我们采用了两部分的方法来提高模型输出与JSON Schema匹配的可靠性。首先，我们训练了最新的模型gpt-4o-2024-08-06，使其理解复杂的模式并且能够最好地生成与之匹配的输出。然而，模型行为本质上是非确定性的——尽管该模型的性能有所提升（在我们的基准测试中达到了93%），但仍未达到开发人员构建可靠应用程序所需的可靠性。因此，我们还采用了一种基于工程的确定性方法来约束模型的输出，以实现100%的可靠性。

#### 约束解码技术
我们的方法基于一种称为约束采样或约束解码的技术。默认情况下，当对模型进行采样以生成输出时，它们是完全不受约束的，可以从词汇表中选择任何token作为下一个输出。这种灵活性使模型容易出错；例如，即使在生成无效JSON时，它们通常也可以随时采样大括号token。为了强制生成有效的输出，我们将模型约束在根据提供的模式有效的token，而不是所有可用的token。

在实践中实现这种约束是具有挑战性的，因为在模型输出的不同阶段，有效token是不同的。假设我们有以下模式：

在输出的开始，有效token包括{, {“, {\n等。然而，一旦模型已经采样了{“val, 那么{就不再是一个有效的token。因此，我们需要实现动态约束解码，并在每个token生成后确定哪些token是有效的，而不是在响应开始时就确定。

为此，我们将提供的JSON Schema转换为上下文无关文法（CFG）。文法是一组定义语言的规则，而上下文无关文法是符合特定规则的文法。可以将JSON和JSON Schema视为具有定义其有效性规则的特定语言。就像在英语中没有动词的句子是无效的一样，在JSON中有尾随逗号也是无效的。

因此，对于每个JSON Schema，我们计算出一个表示该模式的文法，并预处理其组件，以便在模型采样期间易于访问。这就是为什么第一次请求新模式会有延迟的原因——我们必须预处理模式以生成这个在采样期间可以高效使用的工件。

在采样期间，每个token之后，我们的推理引擎将根据先前生成的token和文法中的规则来确定接下来哪些token是有效的。然后我们使用这个token列表来屏蔽下一次采样步骤，有效地将无效token的概率降低到0。由于我们已经预处理了模式，我们可以使用缓存的数据结构高效地完成这一操作，延迟开销最小。

#### 替代方法
解决此问题的替代方法通常使用有限状态机（FSM）或正则表达式（通常由FSM实现）进行约束解码。这些方法的功能类似，因为它们在每个token生成后动态更新哪些token是有效的，但与CFG方法有一些关键区别。值得注意的是，CFG可以表达比FSM更广泛的语言类别。实际上，对于非常简单的模式，这并不重要。然而，我们发现，对于涉及嵌套或递归数据结构的更复杂模式，这一差异是有意义的。举例来说，FSM通常无法表达递归类型，这意味着基于FSM的方法可能难以匹配深度嵌套JSON中的括号。以下是一个支持OpenAI API结构化输出的递归模式示例，但用FSM是无法表达的。

请注意，每个UI元素可以有任意的子元素，这些子元素递归地引用根模式。这种灵活性是CFG方法所提供的。